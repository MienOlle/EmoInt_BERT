{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11833570,"sourceType":"datasetVersion","datasetId":7434328},{"sourceId":11833579,"sourceType":"datasetVersion","datasetId":7434335}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install emoji contractions","metadata":{"id":"DlpZ1l9npHuj","outputId":"b217610d-f68e-49b5-db4c-c2123ac4c8d3","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:05:42.587761Z","iopub.execute_input":"2025-05-19T11:05:42.588682Z","iopub.status.idle":"2025-05-19T11:05:45.460698Z","shell.execute_reply.started":"2025-05-19T11:05:42.588638Z","shell.execute_reply":"2025-05-19T11:05:45.459758Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\nRequirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\nRequirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\nRequirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\nRequirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport re\nimport emoji\nimport contractions\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom nltk.corpus import wordnet\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel, get_scheduler\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nfrom collections import defaultdict","metadata":{"id":"BYpfJN_oniPF","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:05:45.462281Z","iopub.execute_input":"2025-05-19T11:05:45.462560Z","iopub.status.idle":"2025-05-19T11:05:45.468470Z","shell.execute_reply.started":"2025-05-19T11:05:45.462512Z","shell.execute_reply":"2025-05-19T11:05:45.467711Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/elreg-datasets/train.csv', delimiter='\\t')\ntest_df = pd.read_csv('/kaggle/input/elreg-datasets/test.csv', delimiter='\\t')\ndev_df = pd.read_csv('/kaggle/input/elreg-datasets/dev.csv', delimiter='\\t')\n\nprint(f\"Training set: {train_df.shape}\")\nprint(f\"Development set: {dev_df.shape}\")\nprint(f\"Test set: {test_df.shape}\")","metadata":{"id":"RwrzICoHnmnE","outputId":"bf8064be-3966-4592-e585-67afff301ecb","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:05:45.469279Z","iopub.execute_input":"2025-05-19T11:05:45.469526Z","iopub.status.idle":"2025-05-19T11:05:45.570248Z","shell.execute_reply.started":"2025-05-19T11:05:45.469503Z","shell.execute_reply":"2025-05-19T11:05:45.569724Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining set: (6908, 6)\nDevelopment set: (893, 6)\nTest set: (3289, 6)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Text preprocessing functions\ndef convert_emojis(text):\n    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n    text = re.sub(r':([a-zA-Z_]+):', r'\\1', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # expand contractions\n    text = contractions.fix(text)\n    # convert emojis\n    text = convert_emojis(text)\n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    # Remove user mentions and hashtags\n    # text = re.sub(r'\\@\\w+|\\#','', text)\n    text = re.sub(r'@\\w+', '', text)\n    # Remove special characters and numbers (except punctuation)\n    text = re.sub(r\"[^a-zA-Z\\s.,!?']\", '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply text cleaning\ntrain_df[\"clean_text\"] = train_df[\"Tweet\"].apply(clean_text)\ndev_df[\"clean_text\"] = dev_df[\"Tweet\"].apply(clean_text)\ntest_df[\"clean_text\"] = test_df[\"Tweet\"].apply(clean_text)\n\n# Define emotion columns\nemotion_cols = [\"joy\", \"sadness\", \"anger\", \"fear\"]","metadata":{"id":"o5hgrjZgnsJk","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:05:45.571451Z","iopub.execute_input":"2025-05-19T11:05:45.571725Z","iopub.status.idle":"2025-05-19T11:05:46.630879Z","shell.execute_reply.started":"2025-05-19T11:05:45.571707Z","shell.execute_reply":"2025-05-19T11:05:46.630093Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_synonyms(word):\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            # Exclude same word and avoid underscores (multi-word expressions)\n            if lemma.name().lower() != word.lower() and '_' not in lemma.name():\n                synonyms.add(lemma.name().lower())\n    return list(synonyms)\n\ndef safe_synonym_replace(text, replacement_prob=0.3):\n    words = text.split()\n    new_words = []\n\n    for word in words:\n        if random.random() < replacement_prob:\n            synonyms = get_synonyms(word)\n            if synonyms:\n                new_word = random.choice(synonyms)\n                new_words.append(new_word)\n            else:\n                new_words.append(word)\n        else:\n            new_words.append(word)\n\n    return ' '.join(new_words)\n\ndef inject_noise(text, deletion_prob=0.05, swap_prob=0.05):\n    chars = list(text)\n    new_text = []\n\n    i = 0\n    while i < len(chars):\n        if random.random() < deletion_prob:\n            i += 1  # Skip the character (deletion)\n            continue\n\n        if i < len(chars) - 1 and random.random() < swap_prob:\n            # Swap characters\n            new_text.append(chars[i+1])\n            new_text.append(chars[i])\n            i += 2\n            continue\n\n        new_text.append(chars[i])\n        i += 1\n\n    return ''.join(new_text)\n\ndef augment_with_oversampling_and_noise(df, emotion_cols, threshold=2000, replacement_prob=0.3, deletion_prob=0.05, swap_prob=0.05):\n    emotion_counts = df[emotion_cols].gt(0).sum()\n    underrepresented = emotion_counts[emotion_counts < threshold].index.tolist()\n    print(\"Underrepresented emotions:\", underrepresented)\n\n    augmented_rows = []\n\n    for emotion in underrepresented:\n        current_count = emotion_counts[emotion]\n        needed = threshold - current_count\n        print(f\"→ Augmenting {needed} rows for emotion: {emotion}\")\n\n        if needed <= 0:\n            continue\n\n        candidates = df[df[emotion] > 0].sample(n=needed, replace=True, random_state=42).copy()\n\n        half = needed // 2\n        oversample_part = candidates.iloc[:half].copy()\n        noise_part = candidates.iloc[half:].copy()\n\n        oversample_part['clean_text'] = oversample_part['clean_text'].apply(\n            lambda x: inject_noise(\n                safe_synonym_replace(x, replacement_prob),\n                deletion_prob=deletion_prob,\n                swap_prob=swap_prob\n            )\n        )\n\n        # Apply synonym replacement + noise to noise_part\n        noise_part['clean_text'] = noise_part['clean_text'].apply(\n            lambda x: inject_noise(\n                safe_synonym_replace(x, replacement_prob),\n                deletion_prob=deletion_prob,\n                swap_prob=swap_prob\n            )\n        )\n\n        augmented_rows.append(pd.concat([oversample_part, noise_part], ignore_index=True))\n\n    if augmented_rows:\n        df_aug = pd.concat([df] + augmented_rows, ignore_index=True)\n        df_aug = df_aug.sample(frac=1, random_state=42).reset_index(drop=True)\n        print(f\"Final augmented dataset size: {len(df_aug)}\")\n        return df_aug\n    else:\n        print(\"No augmentation was necessary.\")\n        return df\n\nemotion_cols = ['joy', 'sadness', 'anger', 'fear']\ntrain_df = augment_with_oversampling_and_noise(\n    train_df,\n    emotion_cols,\n    threshold = 2000,\n    replacement_prob=0.2,\n    deletion_prob=0.05,\n    swap_prob=0.05\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:05:48.602475Z","iopub.execute_input":"2025-05-19T11:05:48.602793Z","iopub.status.idle":"2025-05-19T11:05:52.220250Z","shell.execute_reply.started":"2025-05-19T11:05:48.602772Z","shell.execute_reply":"2025-05-19T11:05:52.219512Z"}},"outputs":[{"name":"stdout","text":"Underrepresented emotions: ['joy', 'sadness', 'anger']\n→ Augmenting 385 rows for emotion: joy\n→ Augmenting 467 rows for emotion: sadness\n→ Augmenting 299 rows for emotion: anger\nFinal augmented dataset size: 8059\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Check for missing values in emotion columns\nprint(\"Missing values in emotion columns:\")\nfor df, name in [(train_df, \"train\"), (dev_df, \"dev\"), (test_df, \"test\")]:\n    print(f\"\\n{name} dataset:\")\n    for col in emotion_cols:\n        missing = df[col].isna().sum()\n        total = len(df)\n        print(f\"{col}: {missing} missing values ({missing/total*100:.1f}%)\")\n\n# Fill missing values with 0 (indicating absence of that emotion)\nfor df in [train_df, dev_df, test_df]:\n    for col in emotion_cols:\n        df[col] = df[col].fillna(0.0)","metadata":{"id":"XEp6pUbGnwXU","outputId":"29766e1a-d3ed-4841-e595-cd283a397f45","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:05:52.221314Z","iopub.execute_input":"2025-05-19T11:05:52.221534Z","iopub.status.idle":"2025-05-19T11:05:52.233027Z","shell.execute_reply.started":"2025-05-19T11:05:52.221518Z","shell.execute_reply":"2025-05-19T11:05:52.232268Z"}},"outputs":[{"name":"stdout","text":"Missing values in emotion columns:\n\ntrain dataset:\njoy: 0 missing values (0.0%)\nsadness: 0 missing values (0.0%)\nanger: 0 missing values (0.0%)\nfear: 0 missing values (0.0%)\n\ndev dataset:\njoy: 0 missing values (0.0%)\nsadness: 0 missing values (0.0%)\nanger: 0 missing values (0.0%)\nfear: 0 missing values (0.0%)\n\ntest dataset:\njoy: 0 missing values (0.0%)\nsadness: 0 missing values (0.0%)\nanger: 0 missing values (0.0%)\nfear: 0 missing values (0.0%)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Load EmoLex features\ndef load_lex(filepath):\n    lexicon = defaultdict(dict)\n    with open(filepath, 'r') as file:\n        for line in file:\n            word, emotion, value = line.strip().split('\\t')\n            if int(value) == 1:\n                lexicon[word][emotion] = 1\n    return lexicon\n\nnrc_lexicon = load_lex(\"/kaggle/input/nrc-lexicons/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n\ndef extract_lex(text, lexicon):\n    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n              'sadness', 'surprise', 'trust', 'positive', 'negative']\n    counts = dict.fromkeys(emotions, 0)\n\n    for word in text.split():\n        if word in lexicon:\n            for emo in lexicon[word]:\n                counts[emo] += 1\n    return [counts[emo] for emo in emotions]\n\n# Extract lexicon features\ntrain_df['lexicons'] = train_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\ntest_df['lexicons'] = test_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\ndev_df['lexicons'] = dev_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\n\ntrain_lex = np.array(train_df['lexicons'].tolist())\ntest_lex = np.array(test_df['lexicons'].tolist())\ndev_lex = np.array(dev_df['lexicons'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:04.406780Z","iopub.execute_input":"2025-05-19T11:06:04.407051Z","iopub.status.idle":"2025-05-19T11:06:04.549873Z","shell.execute_reply.started":"2025-05-19T11:06:04.407033Z","shell.execute_reply":"2025-05-19T11:06:04.549311Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load VAD Lexicons\ndef load_nrc_vad(filepath):\n    vad_lex = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        next(f)  # skip header\n        for line in f:\n            word, val, aro, dom = line.strip().split('\\t')\n            vad_lex[word] = {\n                'valence': float(val),\n                'arousal': float(aro),\n                'dominance': float(dom)\n            }\n    return vad_lex\n\nnrc_vad_lexicon = load_nrc_vad(\"/kaggle/input/nrc-lexicons/NRC-VAD-Lexicon-v2.1.txt\")\n\ndef extract_vad(text, lexicon):\n    valence = []\n    arousal = []\n    dominance = []\n\n    for word in text.split():\n        if word in lexicon:\n            valence.append(lexicon[word]['valence'])\n            arousal.append(lexicon[word]['arousal'])\n            dominance.append(lexicon[word]['dominance'])\n\n    # If no word matched, return zeros\n    if not valence:\n        return [0.0, 0.0, 0.0]\n\n    # Otherwise, return means\n    return [\n        np.mean(valence),\n        np.mean(arousal),\n        np.mean(dominance)\n    ]\n\n# Extract lexicon features\ntrain_df['vad'] = train_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\ntest_df['vad'] = test_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\ndev_df['vad'] = dev_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\n\ntrain_vad = np.array(train_df['vad'].tolist())\ntest_vad = np.array(test_df['vad'].tolist())\ndev_vad = np.array(dev_df['vad'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:07.218640Z","iopub.execute_input":"2025-05-19T11:06:07.218924Z","iopub.status.idle":"2025-05-19T11:06:07.597045Z","shell.execute_reply.started":"2025-05-19T11:06:07.218905Z","shell.execute_reply":"2025-05-19T11:06:07.596300Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Load HashEmo Lexicons\nfrom collections import defaultdict\n\ndef load_nrc_hash_emo(filepath):\n    lexicon = defaultdict(dict)\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            emotion, word, score = line.strip().split('\\t')\n            lexicon[word][emotion] = float(score)\n    return lexicon\n\nhash_emo_lex = load_nrc_hash_emo('/kaggle/input/nrc-lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n\ndef extract_hash_emo(text, lexicon):\n    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n                'sadness', 'surprise', 'trust']\n    scores = {emo: [] for emo in emotions}\n\n    for word in text.split():\n        if word in lexicon:\n            for emo, value in lexicon[word].items():\n                scores[emo].append(value)\n\n    return [np.mean(scores[emo]) if scores[emo] else 0.0 for emo in emotions]\n\ntrain_df['hash'] = train_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\ntest_df['hash'] = test_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\ndev_df['hash'] = dev_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\n\ntrain_hash = np.array(train_df['hash'].tolist())\ntest_hash = np.array(test_df['hash'].tolist())\ndev_hash = np.array(dev_df['hash'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:09.599625Z","iopub.execute_input":"2025-05-19T11:06:09.599908Z","iopub.status.idle":"2025-05-19T11:06:10.213516Z","shell.execute_reply.started":"2025-05-19T11:06:09.599887Z","shell.execute_reply":"2025-05-19T11:06:10.212979Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"scaler_hash = StandardScaler()\ntrain_hash = scaler_hash.fit_transform(train_hash)\ntest_hash = scaler_hash.transform(test_hash)\ndev_hash = scaler_hash.transform(dev_hash)\n\nscaler_lex = StandardScaler()\ntrain_lex = scaler_lex.fit_transform(train_lex)\ntest_lex = scaler_lex.transform(test_lex)\ndev_lex = scaler_lex.transform(dev_lex)\n\nscaler_vad = StandardScaler()\ntrain_vad = scaler_vad.fit_transform(train_vad)\ntest_vad = scaler_vad.transform(test_vad)\ndev_vad = scaler_vad.transform(dev_vad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:12.948510Z","iopub.execute_input":"2025-05-19T11:06:12.949236Z","iopub.status.idle":"2025-05-19T11:06:12.971988Z","shell.execute_reply.started":"2025-05-19T11:06:12.949211Z","shell.execute_reply":"2025-05-19T11:06:12.971469Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"joblib.dump(scaler_hash, 'hash_scaler.pkl')\njoblib.dump(scaler_lex, 'lex_scaler.pkl')\njoblib.dump(scaler_vad, 'vad_scaler.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:07:00.089871Z","iopub.execute_input":"2025-05-19T11:07:00.090151Z","iopub.status.idle":"2025-05-19T11:07:00.098076Z","shell.execute_reply.started":"2025-05-19T11:07:00.090132Z","shell.execute_reply":"2025-05-19T11:07:00.097442Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['vad_scaler.pkl']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# NRC Hash-Emo + EmoLex + VAD\ntrain_combined = np.concatenate([train_vad, train_lex, train_hash], axis=1)\ntest_combined = np.concatenate([test_vad, test_lex, test_hash], axis=1)\ndev_combined = np.concatenate([dev_vad, dev_lex, dev_hash], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:44.618297Z","iopub.execute_input":"2025-05-19T07:49:44.618558Z","iopub.status.idle":"2025-05-19T07:49:44.623006Z","shell.execute_reply.started":"2025-05-19T07:49:44.618537Z","shell.execute_reply":"2025-05-19T07:49:44.622316Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# Tokenize texts\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_texts(texts, max_len=128):\n    return tokenizer(\n        texts.tolist(),\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors='pt'\n    )\n\ntrain_tokenized = tokenize_texts(train_df['clean_text'])\ntest_tokenized = tokenize_texts(test_df['clean_text'])\ndev_tokenized = tokenize_texts(dev_df['clean_text'])","metadata":{"id":"M4C8I8P_oC0F","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:44.623832Z","iopub.execute_input":"2025-05-19T07:49:44.624235Z","iopub.status.idle":"2025-05-19T07:49:49.469688Z","shell.execute_reply.started":"2025-05-19T07:49:44.624219Z","shell.execute_reply":"2025-05-19T07:49:49.468910Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# Create dataset for multi-label and multi-output learning\nclass EmotionMultiTaskDataset(Dataset):\n    def __init__(self, encodings, emotion_intensities, lexicon_feats=None, texts=None):\n        self.encodings = encodings\n        self.emotion_intensities = emotion_intensities  # DataFrame with emotion columns\n        self.lexicon_feats = lexicon_feats\n        self.texts = texts\n        self.emotion_cols = emotion_intensities.columns.tolist()\n\n    def __len__(self):\n        return len(self.emotion_intensities)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n\n        # Get emotion intensities\n        emotions = self.emotion_intensities.iloc[idx].values\n        item['emotion_intensities'] = torch.tensor(emotions, dtype=torch.float)\n\n        # Binary labels for multi-label classification (1 if emotion present)\n        item['emotion_labels'] = torch.tensor((emotions > 0).astype(int), dtype=torch.float)\n\n        if self.lexicon_feats is not None:\n            item['lexicon_feats'] = torch.tensor(self.lexicon_feats[idx], dtype=torch.float)\n\n        if self.texts is not None:\n            item['text'] = self.texts[idx]\n\n        return item","metadata":{"id":"PR8aYUwYoDPT","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:49.470519Z","iopub.execute_input":"2025-05-19T07:49:49.470795Z","iopub.status.idle":"2025-05-19T07:49:49.476861Z","shell.execute_reply.started":"2025-05-19T07:49:49.470773Z","shell.execute_reply":"2025-05-19T07:49:49.476272Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = EmotionMultiTaskDataset(\n    train_tokenized,\n    train_df[emotion_cols],\n    lexicon_feats=train_combined,\n    texts=train_df['clean_text'].tolist()\n)\n\ndev_dataset = EmotionMultiTaskDataset(\n    dev_tokenized,\n    dev_df[emotion_cols],\n    lexicon_feats=dev_combined,\n    texts=dev_df['clean_text'].tolist()\n)\n\ntest_dataset = EmotionMultiTaskDataset(\n    test_tokenized,\n    test_df[emotion_cols],\n    lexicon_feats=test_combined,\n    texts=test_df['clean_text'].tolist()\n)\n\n# Create dataloaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"5FkQuneXoGcG","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:49.477562Z","iopub.execute_input":"2025-05-19T07:49:49.477798Z","iopub.status.idle":"2025-05-19T07:49:49.495895Z","shell.execute_reply.started":"2025-05-19T07:49:49.477774Z","shell.execute_reply":"2025-05-19T07:49:49.495370Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# Multi-task learning model for multi-label classification and multi-output regression\nclass EmotionMultiTaskModel(nn.Module):\n    def __init__(self, num_emotions=4, lex_dim=10):\n        super(EmotionMultiTaskModel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n\n        # Shared representation\n        hidden_size = self.bert.config.hidden_size\n        self.shared_layer = nn.Linear(hidden_size + lex_dim, hidden_size)\n\n        # Task-specific layers\n        self.classifier = nn.Linear(hidden_size, num_emotions)  # Multi-label classification\n        self.regressor = nn.Linear(hidden_size, num_emotions)   # Multi-output regression\n\n    def forward(self, input_ids, attention_mask, lexicon_feats):\n        # Get BERT embeddings\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n\n        # Concatenate with lexicon features\n        combined = torch.cat((pooled_output, lexicon_feats), dim=1)\n\n        # Shared representation\n        shared_repr = F.relu(self.shared_layer(combined))\n        shared_repr = self.dropout(shared_repr)\n\n        # Task-specific outputs\n        cls_logits = self.classifier(shared_repr)  # For binary classification of each emotion\n        reg_output = self.regressor(shared_repr)   # For regression of each emotion's intensity\n\n        # Apply sigmoid to classification logits\n        cls_probs = torch.sigmoid(cls_logits)\n\n        # Scale regression outputs to [0,1]\n        reg_output = (torch.tanh(reg_output) + 1) / 2\n\n        return cls_probs, reg_output","metadata":{"id":"Pl1A3OXkoIbU","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:49.496590Z","iopub.execute_input":"2025-05-19T07:49:49.496790Z","iopub.status.idle":"2025-05-19T07:49:49.513617Z","shell.execute_reply.started":"2025-05-19T07:49:49.496775Z","shell.execute_reply":"2025-05-19T07:49:49.513039Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# Initialize model\nnum_emotions = len(emotion_cols)\n# lex_dim = train_lex.shape[1]\nlex_dim = train_combined.shape[1]\nmodel = EmotionMultiTaskModel(num_emotions=num_emotions, lex_dim=lex_dim).to(device)\n\n# Loss functions\n# Binary cross-entropy for multi-label classification\ncls_criterion = nn.BCELoss()\n\n# Huber loss for regression\nreg_criterion = nn.HuberLoss(delta=0.3)\n\n# Pearson correlation loss for regression\ndef pearson_loss(preds, targets, epsilon=1e-8):\n    # Apply mask to consider only non-zero targets\n    mask = (targets > 0)\n\n    if not torch.any(mask):\n        return torch.tensor(0.0, device=preds.device)\n\n    preds_masked = preds[mask]\n    targets_masked = targets[mask]\n\n    if len(preds_masked) <= 1:\n        return torch.tensor(0.0, device=preds.device)\n\n    vx = preds_masked - torch.mean(preds_masked)\n    vy = targets_masked - torch.mean(targets_masked)\n\n    corr = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + epsilon)\n    return 1 - corr\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 10\nnum_training_steps = num_epochs * len(train_loader)\nnum_warmup_steps = int(0.1 * num_training_steps)\n\n# Learning rate scheduler\nlr_scheduler = get_scheduler(\n    name=\"cosine_with_restarts\",\n    optimizer=optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\n# Early stopping parameters\npatience = 2\nbest_val_loss = float('inf')\nearly_stop_count = 0","metadata":{"id":"1KiE9XD-oMjb","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:49.514281Z","iopub.execute_input":"2025-05-19T07:49:49.514756Z","iopub.status.idle":"2025-05-19T07:49:49.953277Z","shell.execute_reply.started":"2025-05-19T07:49:49.514734Z","shell.execute_reply":"2025-05-19T07:49:49.952456Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    train_losses = []\n    cls_losses = []\n    reg_losses = []\n\n    loop = tqdm(train_loader, leave=True)\n\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        emotion_labels = batch['emotion_labels'].to(device)\n        emotion_intensities = batch['emotion_intensities'].to(device)\n        lexicon_feats = batch['lexicon_feats'].to(device)\n\n        # Forward pass\n        cls_probs, reg_output = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            lexicon_feats=lexicon_feats\n        )\n\n        # Calculate classification loss (only for emotions that are present or not)\n        cls_loss = cls_criterion(cls_probs, emotion_labels)\n\n        # Calculate regression loss (only for emotions with intensity > 0)\n        # First, standard MSE/Huber loss\n        mask = (emotion_intensities > 0)\n        if torch.any(mask):\n            reg_l1_loss = reg_criterion(reg_output * mask, emotion_intensities)\n            # For Pearson loss, calculate per batch\n            reg_pearson_loss = pearson_loss(reg_output, emotion_intensities)\n            reg_loss = 0.7 * reg_l1_loss + 0.3 * reg_pearson_loss\n        else:\n            reg_loss = torch.tensor(0.0, device=device)\n\n        # Combined loss (with task weighting)\n        loss = 0.3 * cls_loss + 0.7 * reg_loss\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n        # Record losses\n        train_losses.append(loss.item())\n        cls_losses.append(cls_loss.item())\n        reg_losses.append(reg_loss.item())\n\n        # Update progress bar\n        loop.set_description(f\"Epoch {epoch + 1}\")\n        loop.set_postfix(loss=loss.item(), cls_loss=cls_loss.item(), reg_loss=reg_loss.item())\n\n    # Validation\n    model.eval()\n    val_losses = []\n    val_cls_losses = []\n    val_reg_losses = []\n\n    with torch.no_grad():\n        for batch in dev_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            emotion_labels = batch['emotion_labels'].to(device)\n            emotion_intensities = batch['emotion_intensities'].to(device)\n            lexicon_feats = batch['lexicon_feats'].to(device)\n\n            # Forward pass\n            cls_probs, reg_output = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                lexicon_feats=lexicon_feats\n            )\n\n            # Calculate classification loss\n            cls_loss = cls_criterion(cls_probs, emotion_labels)\n\n            # Calculate regression loss\n            mask = (emotion_intensities > 0)\n            if torch.any(mask):\n                reg_l1_loss = reg_criterion(reg_output * mask, emotion_intensities)\n                reg_pearson_loss = pearson_loss(reg_output, emotion_intensities)\n                reg_loss = 0.7 * reg_l1_loss + 0.3 * reg_pearson_loss\n            else:\n                reg_loss = torch.tensor(0.0, device=device)\n\n            # Combined loss\n            loss = 0.3 * cls_loss + 0.7 * reg_loss\n\n            # Record losses\n            val_losses.append(loss.item())\n            val_cls_losses.append(cls_loss.item())\n            val_reg_losses.append(reg_loss.item())\n\n    # Calculate average losses\n    avg_train_loss = sum(train_losses) / len(train_losses)\n    avg_train_cls_loss = sum(cls_losses) / len(cls_losses)\n    avg_train_reg_loss = sum(reg_losses) / len(reg_losses)\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    avg_val_cls_loss = sum(val_cls_losses) / len(val_cls_losses)\n    avg_val_reg_loss = sum(val_reg_losses) / len(val_reg_losses)\n\n    # Print progress\n    print(f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_cls_loss:.4f}, Reg: {avg_train_reg_loss:.4f})\")\n    print(f\"Val Loss: {avg_val_loss:.4f} (Cls: {avg_val_cls_loss:.4f}, Reg: {avg_val_reg_loss:.4f})\")\n\n    # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        early_stop_count = 0\n        # Save best model\n        torch.save(model.state_dict(), \"best_multitask_multilabel_model.pth\")\n        print(\"Model saved!\")\n    else:\n        early_stop_count += 1\n        print(f\"Validation loss did not improve. Early stop counter: {early_stop_count}/{patience}\")\n\n    if early_stop_count >= patience:\n        print(\"Early stopping triggered.\")\n        break","metadata":{"id":"JBpj7DzJnDuT","outputId":"e46d1951-55e5-45b4-f50e-e3cf331235a1","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:49:49.954212Z","iopub.execute_input":"2025-05-19T07:49:49.954475Z","iopub.status.idle":"2025-05-19T07:58:50.087885Z","shell.execute_reply.started":"2025-05-19T07:49:49.954453Z","shell.execute_reply":"2025-05-19T07:58:50.087128Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 504/504 [01:44<00:00,  4.83it/s, cls_loss=0.551, loss=0.251, reg_loss=0.122] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3333 (Cls: 0.6475, Reg: 0.1986)\nVal Loss: 0.2689 (Cls: 0.6615, Reg: 0.1007)\nModel saved!\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 504/504 [01:44<00:00,  4.82it/s, cls_loss=0.474, loss=0.181, reg_loss=0.056] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2162 (Cls: 0.5211, Reg: 0.0855)\nVal Loss: 0.2528 (Cls: 0.6148, Reg: 0.0977)\nModel saved!\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 504/504 [01:44<00:00,  4.83it/s, cls_loss=0.278, loss=0.147, reg_loss=0.0915]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1710 (Cls: 0.4264, Reg: 0.0616)\nVal Loss: 0.2465 (Cls: 0.5843, Reg: 0.1017)\nModel saved!\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 504/504 [01:44<00:00,  4.84it/s, cls_loss=0.411, loss=0.149, reg_loss=0.0359] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1205 (Cls: 0.2801, Reg: 0.0521)\nVal Loss: 0.2580 (Cls: 0.6278, Reg: 0.0995)\nValidation loss did not improve. Early stop counter: 1/2\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 504/504 [01:44<00:00,  4.84it/s, cls_loss=0.19, loss=0.0822, reg_loss=0.0362]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0833 (Cls: 0.1776, Reg: 0.0430)\nVal Loss: 0.2777 (Cls: 0.6994, Reg: 0.0970)\nValidation loss did not improve. Early stop counter: 2/2\nEarly stopping triggered.\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# Load best model for evaluation\nmodel.load_state_dict(torch.load(\"best_multitask_multilabel_model.pth\"))\n\n# Evaluation functions for multi-label classification\ndef evaluate_classification(model, dataloader, device, threshold=0.3):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating classification\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            emotion_labels = batch['emotion_labels']\n            lexicon_feats = batch['lexicon_feats'].to(device)\n\n            # Get classification predictions\n            cls_probs, _ = model(input_ids=input_ids, attention_mask=attention_mask, lexicon_feats=lexicon_feats)\n            preds = (cls_probs > threshold).float().cpu().numpy()\n            # preds = (cls_probs == cls_probs.max(dim=1, keepdim=True).values).float().cpu().numpy()\n\n            all_preds.extend(preds)\n            all_labels.extend(emotion_labels.numpy())\n\n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n\n    # Calculate overall metrics\n    accuracy = accuracy_score(all_labels.flatten(), all_preds.flatten())\n    f1_macro = f1_score(all_labels, all_preds, average='macro')\n    f1_micro = f1_score(all_labels, all_preds, average='micro')\n\n    # Calculate per-emotion metrics\n    per_emotion_f1 = {}\n    for i, emotion in enumerate(emotion_cols):\n        f1 = f1_score(all_labels[:, i], all_preds[:, i])\n        per_emotion_f1[emotion] = f1\n\n    return {\n        \"accuracy\": accuracy,\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"per_emotion_f1\": per_emotion_f1,\n        \"predictions\": all_preds,\n        \"true_labels\": all_labels\n    }\n\n# Evaluation function for regression\ndef evaluate_regression(model, dataloader, device):\n    model.eval()\n    all_preds = []\n    all_true = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating regression\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            emotion_intensities = batch['emotion_intensities']\n            lexicon_feats = batch['lexicon_feats'].to(device)\n\n            # Get regression predictions\n            _, reg_output = model(input_ids=input_ids, attention_mask=attention_mask, lexicon_feats=lexicon_feats)\n\n            all_preds.extend(reg_output.cpu().numpy())\n            all_true.extend(emotion_intensities.numpy())\n\n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_true = np.array(all_true)\n\n    # Calculate overall metrics (for non-zero values)\n    mask = (all_true > 0)\n    if np.any(mask):\n        mse = mean_squared_error(all_true[mask], all_preds[mask])\n        mae = mean_absolute_error(all_true[mask], all_preds[mask])\n        r2 = r2_score(all_true[mask], all_preds[mask])\n    else:\n        mse = mae = r2 = 0\n\n    # Calculate per-emotion metrics\n    per_emotion_metrics = {}\n    for i, emotion in enumerate(emotion_cols):\n        emotion_mask = (all_true[:, i] > 0)\n        if np.sum(emotion_mask) > 1:  # Need at least 2 points for correlation\n            emotion_true = all_true[:, i][emotion_mask]\n            emotion_pred = all_preds[:, i][emotion_mask]\n\n            emotion_mse = mean_squared_error(emotion_true, emotion_pred)\n            emotion_mae = mean_absolute_error(emotion_true, emotion_pred)\n            try:\n                emotion_pearson, _ = pearsonr(emotion_true, emotion_pred)\n            except:\n                emotion_pearson = float('nan')\n\n            per_emotion_metrics[emotion] = {\n                \"mse\": emotion_mse,\n                \"mae\": emotion_mae,\n                \"pearson\": emotion_pearson\n            }\n        else:\n            per_emotion_metrics[emotion] = {\n                \"mse\": float('nan'),\n                \"mae\": float('nan'),\n                \"pearson\": float('nan')\n            }\n\n    # Compute average Pearson (excluding NaNs)\n    pearson_values = [metrics[\"pearson\"] for metrics in per_emotion_metrics.values() if not np.isnan(metrics[\"pearson\"])]\n    avg_pearson = np.mean(pearson_values) if pearson_values else float('nan')\n\n    return {\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n        \"avg_pearson\": avg_pearson,\n        \"per_emotion_metrics\": per_emotion_metrics,\n        \"predictions\": all_preds,\n        \"true_values\": all_true\n    }\n\n# Evaluate the model\nprint(\"\\n--- Multi-Label Classification Results ---\")\ncls_results = evaluate_classification(model, test_loader, device)\nprint(f\"Test Accuracy: {cls_results['accuracy']:.4f}\")\nprint(f\"F1 Macro: {cls_results['f1_macro']:.4f}\")\nprint(f\"F1 Micro: {cls_results['f1_micro']:.4f}\")\n\nprint(\"\\nPer-emotion F1 scores:\")\nfor emotion, f1 in cls_results['per_emotion_f1'].items():\n    print(f\"{emotion}: {f1:.4f}\")\n\nprint(\"\\n--- Regression Results ---\")\nreg_results = evaluate_regression(model, test_loader, device)\nprint(f\"MSE: {reg_results['mse']:.4f}\")\nprint(f\"MAE: {reg_results['mae']:.4f}\")\nprint(f\"R²: {reg_results['r2']:.4f}\")\n\nprint(\"\\nPer-emotion regression metrics:\")\nfor emotion, metrics in reg_results['per_emotion_metrics'].items():\n    print(f\"{emotion}:\")\n    print(f\"  MSE: {metrics['mse']:.4f}\")\n    print(f\"  MAE: {metrics['mae']:.4f}\")\n    print(f\"  Pearson: {metrics['pearson']:.4f}\")\n\nprint(f\"Avg Pearson: {reg_results['avg_pearson']:.4f}\")","metadata":{"id":"yqYWqJjWwW_h","outputId":"36777e8d-0e56-43d7-90cc-13f235fc774b","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:58:50.088992Z","iopub.execute_input":"2025-05-19T07:58:50.089506Z","iopub.status.idle":"2025-05-19T07:59:13.106558Z","shell.execute_reply.started":"2025-05-19T07:58:50.089487Z","shell.execute_reply":"2025-05-19T07:59:13.105980Z"}},"outputs":[{"name":"stdout","text":"\n--- Multi-Label Classification Results ---\n","output_type":"stream"},{"name":"stderr","text":"Evaluating classification: 100%|██████████| 206/206 [00:11<00:00, 18.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.8000\nF1 Macro: 0.6894\nF1 Micro: 0.6855\n\nPer-emotion F1 scores:\njoy: 0.8002\nsadness: 0.6208\nanger: 0.7102\nfear: 0.6265\n\n--- Regression Results ---\n","output_type":"stream"},{"name":"stderr","text":"Evaluating regression: 100%|██████████| 206/206 [00:11<00:00, 18.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"MSE: 0.0256\nMAE: 0.1264\nR²: 0.2557\n\nPer-emotion regression metrics:\njoy:\n  MSE: 0.0295\n  MAE: 0.1371\n  Pearson: 0.7234\nsadness:\n  MSE: 0.0323\n  MAE: 0.1443\n  Pearson: 0.7181\nanger:\n  MSE: 0.0169\n  MAE: 0.1033\n  Pearson: 0.7578\nfear:\n  MSE: 0.0234\n  MAE: 0.1202\n  Pearson: 0.7430\nAvg Pearson: 0.7356\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# Generate predictions for new text\ndef extract_all_lexicons(text):\n    vad_feats = extract_vad(text, nrc_vad_lexicon)\n    lex_feats = extract_lex(text, nrc_lexicon)\n    hash_feats = extract_hash_emo(text, hash_emo_lex)\n    \n    combined_feats = np.concatenate([vad_feats, lex_feats, hash_feats])\n    return combined_feats\n\ndef predict_emotions(text, model, tokenizer, threshold=0.3):\n    model.eval()\n\n    # Clean and tokenize the text\n    clean = clean_text(text)\n    tokens = tokenizer(\n        clean,\n        padding='max_length',\n        truncation=True,\n        max_length=128,\n        return_tensors='pt'\n    )\n\n    # Create lexicon features\n    lexicon_feats = torch.tensor([extract_all_lexicons(clean)], dtype=torch.float).to(device)\n\n    # Move inputs to device\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    # Get predictions\n    with torch.no_grad():\n        cls_probs, intensities = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            lexicon_feats=lexicon_feats\n        )\n\n        # Convert to numpy\n        cls_probs = cls_probs.cpu().numpy()[0]\n        intensities = intensities.cpu().numpy()[0]\n\n        # Apply threshold to classification probabilities\n        # detected_emotions = cls_probs > threshold\n        detected_emotions = np.zeros_like(cls_probs, dtype=bool)\n        detected_emotions[cls_probs.argmax()] = True\n\n    # Prepare results\n    results = {}\n    for i, emotion in enumerate(emotion_cols):\n        results[emotion] = {\n            \"probability\": float(cls_probs[i]),\n            \"detected\": bool(detected_emotions[i]),\n            \"intensity\": float(intensities[i]) if detected_emotions[i] else 0.0\n        }\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:59:13.107273Z","iopub.execute_input":"2025-05-19T07:59:13.107570Z","iopub.status.idle":"2025-05-19T07:59:13.114639Z","shell.execute_reply.started":"2025-05-19T07:59:13.107532Z","shell.execute_reply":"2025-05-19T07:59:13.113876Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Demo with sample texts\nsample_texts = [\n    \"I'm so happy today! Everything is going well.\",\n    \"This makes me so angry, I can't believe they did that.\",\n    \"I'm angry, but I think I can tolerate their behavior.\",\n    \"I might be happy today, but it's just a normal day.\",\n    \"I'm feeling a bit down today, things aren't going as planned.\",\n    \"My girlfriend just dumped me, I don't know what to do with my life anymore. I'm in agony.\",\n    \"That movie was terrifying, I couldn't sleep all night.\"\n]\n\nprint(\"\\n--- Sample Predictions ---\")\nfor text in sample_texts:\n    result = predict_emotions(text, model, tokenizer)\n    print(f\"Text: {text}\")\n    print(\"Detected emotions:\")\n\n    # Sort emotions by intensity\n    emotions_sorted = sorted(\n        [(emotion, details) for emotion, details in result.items() if details[\"detected\"]],\n        key=lambda x: x[1][\"intensity\"],\n        reverse=True\n    )\n\n    if emotions_sorted:\n        for emotion, details in emotions_sorted:\n            print(f\"  {emotion}: intensity={details['intensity']:.2f}, probability={details['probability']:.2f}\")\n    else:\n        print(\"  No emotions detected\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:59:13.115228Z","iopub.execute_input":"2025-05-19T07:59:13.115433Z","iopub.status.idle":"2025-05-19T07:59:13.210035Z","shell.execute_reply.started":"2025-05-19T07:59:13.115410Z","shell.execute_reply":"2025-05-19T07:59:13.209341Z"}},"outputs":[{"name":"stdout","text":"\n--- Sample Predictions ---\nText: I'm so happy today! Everything is going well.\nDetected emotions:\n  joy: intensity=0.87, probability=0.89\n---\nText: This makes me so angry, I can't believe they did that.\nDetected emotions:\n  anger: intensity=0.76, probability=0.89\n---\nText: I'm angry, but I think I can tolerate their behavior.\nDetected emotions:\n  anger: intensity=0.53, probability=0.88\n---\nText: I might be happy today, but it's just a normal day.\nDetected emotions:\n  joy: intensity=0.50, probability=0.73\n---\nText: I'm feeling a bit down today, things aren't going as planned.\nDetected emotions:\n  fear: intensity=0.80, probability=0.56\n---\nText: My girlfriend just dumped me, I don't know what to do with my life anymore. I'm in agony.\nDetected emotions:\n  fear: intensity=0.88, probability=0.47\n---\nText: That movie was terrifying, I couldn't sleep all night.\nDetected emotions:\n  fear: intensity=0.87, probability=0.70\n---\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}